"""
Tests autonomes des amÃ©liorations SophIA - VERSION AVEC TOUS LES LOGS
Version complÃ¨te avec debug dÃ©taillÃ© pour diagnostiquer le freeze
"""

import sys
import os
import time
import logging

# CONFIGURATION LOGGING COMPLÃˆTE EN PREMIER
print("ğŸ”§ Configuration du logging...")
logging.basicConfig(
    level=logging.DEBUG,
    format='ğŸ•’ %(asctime)s | ğŸ“¦ %(name)20s | ğŸ” %(levelname)8s | ğŸ’¬ %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
    ]
)

# Activation de TOUS les logs SophIA
loggers_to_activate = [
    'sophia',
    'sophia.bridge', 
    'sophia.bridge.concept_text_bridge',
    'sophia.llm',
    'sophia.llm.llama_interface', 
    'sophia.constraints',
    'sophia.core',
    'sophia.core.ontology',
    'requests',
    'urllib3'
]

for logger_name in loggers_to_activate:
    logger = logging.getLogger(logger_name)
    logger.setLevel(logging.DEBUG)
    logger.propagate = True

print("âœ… Logging configurÃ© pour tous les modules SophIA")

# Configuration des imports
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, ROOT_DIR)
print(f"ğŸ“ ROOT_DIR configurÃ©: {ROOT_DIR}")

# Utilitaires de test intÃ©grÃ©s
def assert_valid_response(response, min_length=10):
    """VÃ©rifie qu'une rÃ©ponse SophIA est valide"""
    print(f"ğŸ” Validation rÃ©ponse (min_length={min_length})")
    assert response is not None
    assert hasattr(response, 'natural_response')
    assert hasattr(response, 'conceptual_analysis')
    assert len(response.natural_response) >= min_length
    assert 'concepts_detected' in response.conceptual_analysis
    print("âœ… RÃ©ponse valide")

def assert_concepts_detected(response, expected_concepts, min_match=1):
    """VÃ©rifie que les concepts attendus sont dÃ©tectÃ©s"""
    print(f"ğŸ” VÃ©rification concepts (attendus: {expected_concepts}, min_match={min_match})")
    detected = response.conceptual_analysis.get('concepts_detected', [])
    matches = sum(1 for concept in expected_concepts if concept in detected)
    assert matches >= min_match, f"Expected at least {min_match} concepts from {expected_concepts}, got {detected}"
    print(f"âœ… {matches} concepts trouvÃ©s sur {len(expected_concepts)} attendus")

def measure_response_time(func, *args, **kwargs):
    """Mesure le temps de rÃ©ponse d'une fonction"""
    print(f"â±ï¸ DÃ©but mesure temps pour {func.__name__}")
    start = time.time()
    result = func(*args, **kwargs)
    end = time.time()
    duration = end - start
    print(f"â±ï¸ Fin mesure: {duration:.2f}s")
    return result, duration

def test_concept_bridge():
    """Teste le pont conceptuel amÃ©liorÃ©"""
    print("\n" + "="*60)
    print("ğŸ”— DÃ‰BUT TEST CONCEPT BRIDGE")
    print("="*60)
    
    try:
        print("ğŸ“¥ Import des modules Concept Bridge...")
        print("  ğŸ“¦ Import EnhancedConceptTextBridge...")
        from sophia.bridge.concept_text_bridge import EnhancedConceptTextBridge
        print("  âœ… EnhancedConceptTextBridge importÃ©")
        
        print("  ğŸ“¦ Import SimpleOntology...")
        from sophia.core.ontology import SimpleOntology
        print("  âœ… SimpleOntology importÃ©")
        
        print("  ğŸ“¦ Import OllamaLLaMAInterface...")
        from sophia.llm.llama_interface import OllamaLLaMAInterface
        print("  âœ… OllamaLLaMAInterface importÃ©")
        
        print("\nğŸ—ï¸ CrÃ©ation des composants...")
        
        print("  ğŸ—ï¸ CrÃ©ation ontologie...")
        start_time = time.time()
        ontology = SimpleOntology()
        ontology_time = time.time() - start_time
        print(f"  âœ… Ontologie crÃ©Ã©e en {ontology_time:.3f}s avec {len(ontology.concepts)} concepts")
        
        print("  ğŸ¦™ CrÃ©ation interface LLaMA...")
        start_time = time.time()
        llm = OllamaLLaMAInterface()
        llm_time = time.time() - start_time
        print(f"  âœ… LLaMA interface crÃ©Ã©e en {llm_time:.3f}s")
        
        # VÃ©rification status LLaMA AVANT d'utiliser
        print("  ğŸ” VÃ©rification status LLaMA...")
        try:
            llm_info = llm.get_model_info()
            print(f"  ğŸ“Š LLaMA Info complÃ¨te: {llm_info}")
            print(f"  ğŸ“Š LLaMA disponible: {getattr(llm, 'available', 'N/A')}")
            print(f"  ğŸ“Š LLaMA status: {llm_info.get('status', 'N/A')}")
        except Exception as e:
            print(f"  âš ï¸ Erreur rÃ©cupÃ©ration info LLaMA: {e}")
        
        print("\nğŸŒ‰ CRÃ‰ATION DU CONCEPT BRIDGE...")
        print("âš ï¸  ATTENTION: Cette Ã©tape peut prendre du temps ou freezer!")
        print("âš ï¸  Appel Ã  Ollama pour gÃ©nÃ©ration de synonymes...")
        
        start_time = time.time()
        print(f"â° DÃ©but crÃ©ation bridge: {time.strftime('%H:%M:%S')}")
        
        # CrÃ©ation avec suivi dÃ©taillÃ©
        bridge = EnhancedConceptTextBridge(ontology, llm)
        
        bridge_time = time.time() - start_time
        print(f"âœ… Bridge crÃ©Ã© en {bridge_time:.2f}s")
        
        # Test d'initialisation
        print("\nğŸ” VÃ©rification initialisation bridge...")
        assert bridge.ontology is not None
        print("  âœ… Ontologie attachÃ©e")
        assert bridge.llm is not None
        print("  âœ… LLM attachÃ©")
        assert len(bridge.concept_patterns) > 0
        print(f"  âœ… {len(bridge.concept_patterns)} patterns de concepts")
        assert hasattr(bridge, '_synonyms_cache')
        print(f"  âœ… Cache synonymes: {len(bridge._synonyms_cache)} entrÃ©es")
        assert hasattr(bridge, '_context_cache')
        print(f"  âœ… Cache contexte: {len(bridge._context_cache)} entrÃ©es")
        
        # Test des synonymes amÃ©liorÃ©s
        print("\nğŸ” Test gÃ©nÃ©ration synonymes...")
        test_concepts = ['VÃ‰RITÃ‰', 'JUSTICE', 'LIBERTÃ‰']
        
        for concept in test_concepts:
            print(f"  ğŸ” Test synonymes pour {concept}...")
            start_time = time.time()
            
            synonyms = bridge._get_concept_synonyms(concept)
            
            synonyms_time = time.time() - start_time
            print(f"  âœ… Synonymes {concept} gÃ©nÃ©rÃ©s en {synonyms_time:.2f}s: {synonyms[:3]}...")
        
        # Test de l'extraction amÃ©liorÃ©e
        print("\nğŸ” Test extraction amÃ©liorÃ©e...")
        test_text = "La vÃ©ritÃ© philosophique est-elle relative Ã  notre perception de la justice ?"
        base_extraction = {'concepts_detected': ['VÃ‰RITÃ‰'], 'confidence': 0.8}
        
        print(f"  ğŸ“ Texte test: {test_text}")
        print(f"  ğŸ“ Extraction base: {base_extraction}")
        
        start_time = time.time()
        enhanced = bridge.enhanced_concept_extraction(test_text, base_extraction)
        extraction_time = time.time() - start_time
        
        print(f"  âœ… Extraction amÃ©liorÃ©e en {extraction_time:.2f}s")
        
        # VÃ©rifications
        assert 'enhanced_confidence' in enhanced
        print(f"  âœ… Confiance amÃ©liorÃ©e: {enhanced['enhanced_confidence']:.3f}")
        
        assert 'concept_matches' in enhanced
        print(f"  âœ… Matches trouvÃ©s: {len(enhanced['concept_matches'])}")
        
        assert 'concepts_detailed' in enhanced
        print(f"  âœ… Concepts dÃ©taillÃ©s: {len(enhanced['concepts_detailed'])}")
        
        assert enhanced['enhanced_confidence'] >= 0.0
        assert enhanced['enhanced_confidence'] <= 1.0
        print("  âœ… Confiance dans les limites [0,1]")

        print(f"\nğŸ“Š RÃ‰SULTATS BRIDGE:")
        print(f"  ğŸ¯ Concepts dÃ©tectÃ©s: {enhanced['concepts_detected']}")
        print(f"  ğŸ¯ Confiance amÃ©liorÃ©e: {enhanced['enhanced_confidence']:.3f}")
        print(f"  ğŸ¯ Temps total: {bridge_time + extraction_time:.2f}s")
        
        print("âœ… TEST CONCEPT BRIDGE RÃ‰USSI")
        return True
        
    except Exception as e:
        print(f"\nâŒ ERREUR CONCEPT BRIDGE: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_constraint_manager():
    """Teste le gestionnaire de contraintes"""
    print("\n" + "="*60)
    print("âš–ï¸ DÃ‰BUT TEST CONSTRAINT MANAGER")
    print("="*60)
    
    try:
        print("ğŸ“¥ Import des modules Constraint Manager...")
        from sophia.constraints.constraint_manager import PhilosophicalConstraintManager
        from sophia.core.ontology import SimpleOntology
        from sophia.llm.llama_interface import OllamaLLaMAInterface
        print("âœ… Imports rÃ©ussis")
        
        print("\nğŸ—ï¸ CrÃ©ation des composants...")
        ontology = SimpleOntology()
        llm = OllamaLLaMAInterface()
        print("âœ… Composants crÃ©Ã©s")
        
        print("âš–ï¸ CrÃ©ation Constraint Manager...")
        start_time = time.time()
        constraint_manager = PhilosophicalConstraintManager(ontology, llm)
        manager_time = time.time() - start_time
        print(f"âœ… Manager crÃ©Ã© en {manager_time:.2f}s")
        
        # Test d'initialisation
        print("\nğŸ” VÃ©rification initialisation...")
        assert constraint_manager.ontology is not None
        print("  âœ… Ontologie attachÃ©e")
        assert constraint_manager.llm is not None
        print("  âœ… LLM attachÃ©")
        assert len(constraint_manager.constraints) > 0
        print(f"  âœ… {len(constraint_manager.constraints)} contraintes")
        assert len(constraint_manager.philosophical_clusters) > 0
        print(f"  âœ… {len(constraint_manager.philosophical_clusters)} clusters")
        
        # Test des clusters philosophiques
        print("\nğŸ” Test des clusters philosophiques...")
        clusters = constraint_manager.philosophical_clusters
        cluster_names = [c['name'] for c in clusters]
        
        expected_clusters = ['Ã‰pistÃ©mologie', 'Ã‰thique', 'MÃ©taphysique']
        found_clusters = [name for name in expected_clusters if name in cluster_names]
        
        print(f"  ğŸ“Š Clusters attendus: {expected_clusters}")
        print(f"  ğŸ“Š Clusters trouvÃ©s: {found_clusters}")
        print(f"  ğŸ“Š Total clusters: {len(clusters)}")
        
        for cluster in clusters:
            print(f"    ğŸ·ï¸ {cluster['name']}: {len(cluster['concepts'])} concepts, {len(cluster['themes'])} thÃ¨mes")
        
        # Test de validation
        print("\nğŸ” Test validation rÃ©ponse...")
        test_response = """La vÃ©ritÃ© est un concept philosophique fondamental. 
        Tout d'abord, elle peut Ãªtre considÃ©rÃ©e comme absolue selon Platon. 
        Ensuite, elle peut Ãªtre relative selon les sophistes. 
        Enfin, elle dÃ©pend de notre capacitÃ© Ã  connaÃ®tre la rÃ©alitÃ©."""
        
        test_context = {
            'concepts_detected': ['VÃ‰RITÃ‰', 'CONNAISSANCE'],
            'question': 'Qu\'est-ce que la vÃ©ritÃ© ?'
        }
        
        print(f"  ğŸ“ RÃ©ponse test: {test_response[:50]}...")
        print(f"  ğŸ“ Contexte: {test_context}")
        
        start_time = time.time()
        validation = constraint_manager.validate_response(test_response, test_context)
        validation_time = time.time() - start_time
        print(f"  âœ… Validation en {validation_time:.2f}s")
        
        # VÃ©rifications
        assert 'global_score' in validation
        assert 'constraint_results' in validation
        assert 'is_valid' in validation
        assert 0.0 <= validation['global_score'] <= 1.0
        
        print(f"\nğŸ“Š RÃ‰SULTATS VALIDATION:")
        print(f"  ğŸ¯ Score global: {validation['global_score']:.3f}")
        print(f"  ğŸ¯ Contraintes actives: {len(constraint_manager.constraints)}")
        print(f"  ğŸ¯ Validation rÃ©ussie: {validation['is_valid']}")
        print(f"  ğŸ¯ Violations: {len(validation.get('violations', []))}")
        
        if validation.get('recommendations'):
            print(f"  ğŸ’¡ Recommandations: {len(validation['recommendations'])}")
            for rec in validation['recommendations'][:2]:
                print(f"    ğŸ’¡ {rec}")
        
        print("âœ… TEST CONSTRAINT MANAGER RÃ‰USSI")
        return True
        
    except Exception as e:
        print(f"\nâŒ ERREUR CONSTRAINT MANAGER: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_enhanced_sophia():
    """Teste SophIA avec les amÃ©liorations"""
    print("\n" + "="*60)
    print("ğŸ§  DÃ‰BUT TEST SOPHIA ENHANCED")
    print("="*60)
    
    try:
        print("ğŸ“¥ Import SophIA Enhanced...")
        from sophia.core.sophia_hybrid import HybridSophIA
        print("âœ… Import rÃ©ussi")
        
        print("\nğŸš€ Initialisation SophIA Enhanced...")
        print("âš ï¸  Cette Ã©tape peut prendre du temps (crÃ©ation bridge + constraints)...")
        
        start_time = time.time()
        sophia = HybridSophIA(session_name="test_enhanced", auto_save=False)
        init_time = time.time() - start_time
        print(f"âœ… SophIA Enhanced initialisÃ©e en {init_time:.2f}s")
        
        # VÃ©rifications d'initialisation
        print("\nğŸ” VÃ©rification des composants...")
        assert hasattr(sophia, 'concept_bridge')
        print("  âœ… Concept bridge prÃ©sent")
        assert hasattr(sophia, 'constraint_manager')
        print("  âœ… Constraint manager prÃ©sent")
        assert sophia.concept_bridge is not None
        print("  âœ… Concept bridge initialisÃ©")
        assert sophia.constraint_manager is not None
        print("  âœ… Constraint manager initialisÃ©")
        
        print(f"\nğŸ“Š COMPOSANTS SOPHIA:")
        print(f"  ğŸ“š Ontologie: {len(sophia.ontology.concepts)} concepts")
        print(f"  ğŸ”— Bridge cache: {len(getattr(sophia.concept_bridge, '_synonyms_cache', {}))}")
        print(f"  âš–ï¸ Contraintes: {len(sophia.constraint_manager.constraints)}")
        print(f"  ğŸ¯ Clusters: {len(sophia.constraint_manager.philosophical_clusters)}")
        
        # Test de conversation
        print("\nğŸ’¬ Test de conversation...")
        questions = [
            "Qu'est-ce que la vÃ©ritÃ© ?",
            "La justice peut-elle Ãªtre absolue ?"
        ]
        
        for i, question in enumerate(questions, 1):
            print(f"\nğŸ“ Question {i}: {question}")
            
            start_time = time.time()
            response = sophia.ask(question)
            response_time = time.time() - start_time
            
            print(f"â±ï¸ RÃ©ponse gÃ©nÃ©rÃ©e en {response_time:.2f}s")
            
            # VÃ©rifications de base
            assert_valid_response(response, min_length=30)
            assert hasattr(response, 'validation_report')
            assert response.confidence > 0.0
            
            print(f"  ğŸ§  RÃ©ponse: {response.natural_response[:100]}...")
            
            concepts = response.conceptual_analysis.get('concepts_detected', [])
            print(f"  ğŸ’¡ Concepts: {concepts}")
            print(f"  ğŸ¯ Confiance: {response.confidence:.3f}")
            
            if 'global_score' in response.validation_report:
                print(f"  ğŸ“Š Score validation: {response.validation_report['global_score']:.3f}")
            
            # DÃ©tails supplÃ©mentaires
            if hasattr(response, 'lcm_reasoning'):
                lcm_paths = response.lcm_reasoning.get('reasoning_paths', [])
                print(f"  ğŸ§  Chemins LCM: {len(lcm_paths)}")
        
        print("âœ… TEST SOPHIA ENHANCED RÃ‰USSI")
        return True
        
    except Exception as e:
        print(f"\nâŒ ERREUR SOPHIA ENHANCED: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_performance():
    """Teste les performances"""
    print("\n" + "="*60)
    print("âš¡ DÃ‰BUT TEST PERFORMANCE")
    print("="*60)
    
    try:
        print("ğŸš€ Initialisation pour test performance...")
        from sophia.core.sophia_hybrid import HybridSophIA
        
        start_time = time.time()
        sophia = HybridSophIA(session_name="test_perf", auto_save=False)
        init_time = time.time() - start_time
        print(f"âœ… Initialisation en {init_time:.2f}s")
        
        question = "Qu'est-ce que la vÃ©ritÃ© ?"
        print(f"ğŸ“ Question test: {question}")
        
        response, duration = measure_response_time(sophia.ask, question)
        
        assert_valid_response(response)
        print(f"âœ… RÃ©ponse valide gÃ©nÃ©rÃ©e")
        
        # Ã‰valuation performance
        if duration < 10.0:
            perf_level = "ğŸš€ EXCELLENTE"
        elif duration < 30.0:
            perf_level = "âœ… ACCEPTABLE"
        elif duration < 60.0:
            perf_level = "âš ï¸ LENTE"
        else:
            perf_level = "âŒ TRÃˆS LENTE"
        
        print(f"\nğŸ“Š RÃ‰SULTATS PERFORMANCE:")
        print(f"  â±ï¸ Temps de rÃ©ponse: {duration:.2f}s")
        print(f"  ğŸ“ˆ Niveau: {perf_level}")
        print(f"  ğŸ¯ Seuils: <10s=Excellent, <30s=Acceptable, <60s=Lent")
        
        print("âœ… TEST PERFORMANCE TERMINÃ‰")
        return True
        
    except Exception as e:
        print(f"\nâŒ ERREUR PERFORMANCE: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Test principal avec logs complets"""
    print("ğŸ§ª TESTS DES AMÃ‰LIORATIONS SOPHIA - VERSION LOGS COMPLETS")
    print("=" * 80)
    print(f"ğŸ•’ DÃ©but des tests: {time.strftime('%H:%M:%S')}")
    print(f"ğŸ“ RÃ©pertoire racine: {ROOT_DIR}")
    print("=" * 80)
    
    tests = [
        ("Concept Bridge", test_concept_bridge),
        ("Constraint Manager", test_constraint_manager),
        ("SophIA Enhanced", test_enhanced_sophia),
        ("Performance", test_performance)
    ]
    
    results = []
    total_start_time = time.time()
    
    for test_name, test_func in tests:
        print(f"\nğŸš€ LANCEMENT: {test_name}")
        print("ğŸ•’ DÃ©but:", time.strftime('%H:%M:%S'))
        print("-" * 70)
        
        test_start_time = time.time()
        
        try:
            success = test_func()
            test_duration = time.time() - test_start_time
            results.append((test_name, success, test_duration))
            
            status = "âœ… RÃ‰USSI" if success else "âŒ Ã‰CHOUÃ‰"
            print(f"\nğŸ“Š {test_name}: {status} (â±ï¸ {test_duration:.2f}s)")
            
        except Exception as e:
            test_duration = time.time() - test_start_time
            print(f"\nğŸ’¥ {test_name}: ERREUR FATALE - {e}")
            print(f"â±ï¸ Temps avant erreur: {test_duration:.2f}s")
            import traceback
            traceback.print_exc()
            results.append((test_name, False, test_duration))
    
    # RÃ©sumÃ© final
    total_duration = time.time() - total_start_time
    
    print("\n" + "=" * 80)
    print("ğŸ“Š RÃ‰SUMÃ‰ FINAL DES TESTS")
    print("=" * 80)
    print(f"ğŸ•’ Temps total: {total_duration:.2f}s")
    print("-" * 40)
    
    passed = 0
    total = len(results)
    
    for test_name, success, duration in results:
        status = "âœ… RÃ‰USSI" if success else "âŒ Ã‰CHOUÃ‰"
        print(f"{test_name:<20} {status} (â±ï¸ {duration:6.2f}s)")
        if success:
            passed += 1
    
    print("-" * 40)
    print(f"ğŸ¯ SCORE FINAL: {passed}/{total} tests rÃ©ussis ({passed/total*100:.0f}%)")
    
    if passed == total:
        print("\nğŸ‰ TOUTES LES AMÃ‰LIORATIONS FONCTIONNENT PARFAITEMENT !")
        print("\nğŸš€ Prochaines Ã©tapes:")
        print("1. Lance: python simple_interface.py")
        print("2. Lance: python launch_web.py") 
        print("3. Teste avec des questions complexes!")
        print("4. Observe les amÃ©liorations (synonymes, validation, etc.)")
    elif passed > 0:
        print(f"\nâœ… {passed} amÃ©liorations fonctionnent, {total-passed} ont des problÃ¨mes.")
        print("VÃ©rifiez les logs dÃ©taillÃ©s ci-dessus pour diagnostiquer.")
    else:
        print("\nâŒ Aucune amÃ©lioration ne fonctionne.")
        print("VÃ©rifiez que Ollama est lancÃ© et que les modules sont bien crÃ©Ã©s.")
    
    print(f"\nğŸ•’ Fin des tests: {time.strftime('%H:%M:%S')}")
    return passed == total

if __name__ == '__main__':
    success = main()
    print(f"\n{'ğŸ‰ SUCCÃˆS TOTAL' if success else 'âš ï¸ Ã‰CHECS DÃ‰TECTÃ‰S'} - Tests terminÃ©s")
    sys.exit(0 if success else 1)